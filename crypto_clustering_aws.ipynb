{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "import dataframe_image as dfi\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "import mxnet as mx\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# SK Learn\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "# Imbalanced Learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn import over_sampling as os\n",
    "from imblearn import pipeline as pl\n",
    "from imblearn.metrics import (geometric_mean_score, make_index_balanced_accuracy)\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "# AWS Sagemaker \n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import PCA\n",
    "from sagemaker import KMeans\n",
    "import boto3  \n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Sagemaker client \n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Retrieve ARN from AWS\n",
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in AWS S3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate AWS client\n",
    "## S3 policy revision\n",
    "s3_client = boto3.client(\"s3\")\n",
    "data_bucket_name = \"unit-13-challenge\"\n",
    "\n",
    "# Retrieve ARN / execution role \n",
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve list of objects contained within s3 bucket\n",
    "obj_list = s3_client.list_objects(Bucket=data_bucket_name)\n",
    "file = ['crypto_data.csv']\n",
    "for contents in obj_list[\"Contents\"]:\n",
    "    file.append(contents[\"Key\"])\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from the CSV file in s3 bucket\n",
    "response = s3_client.get_object(Bucket=data_bucket_name, Key=file_data)\n",
    "response_body = response[\"Body\"].read()\n",
    "\n",
    "# Create dataframe\n",
    "crypto_data = pd.read_csv(io.BytesIO(response_body), header=0, delimiter=\",\", low_memory=False)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only necessary columns:\n",
    "# 'CoinName','Algorithm','IsTrading','ProofType','TotalCoinsMined','TotalCoinSupply'\n",
    "crypto_data.drop(crypto_data.columns[crypto_data.columns.str.contains('unnamed', case = False)], axis = 1, inplace = True)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only cryptocurrencies that are trading\n",
    "crypto_data.drop(crypto_data[crypto_data.IsTrading=='False'].index, inplace = True)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only cryptocurrencies with a working algorithm\n",
    "crypto_data = crypto_data.loc[crypto_data['IsTrading'] == True]\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"IsTrading\" column\n",
    "crypto_data.drop(columns = 'IsTrading', axis = 1, inplace = True)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values \n",
    "crypto_data.dropna(inplace=True)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with cryptocurrencies having no coins mined\n",
    "crypt_data = crypto_data.loc[crypto_data['TotalCoinsMined'] != 0]\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_data.dropna()\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_data.index = crypto_data[\"CoinName\"]\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_data.drop(crypto_data.columns[crypto_data.columns.str.contains('CoinName', case = False)], axis = 1, inplace = True)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [\"TotalCoinsMined\", \"TotalCoinSupply\"]:\n",
    "    ax = plt.subplots(figsize=(10, 4))\n",
    "    ax = sns.distplot(crypto_data[a])\n",
    "    title = \"Histogram of \" + a\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "# for a in [\"CoinName\", \"TotalCoinsMined\", \"TotalCoinSupply\"]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scaling- We need to standardize the scaling of the numerical columns in order to use any distance based analytical methods so that we can compare the relative distances between different feature columns. We can use minmaxscaler to transform the numerical columns so that they also fall between 0 and 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler   \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(crypto_data))\n",
    "df_scaled.columns = crypto_data.columns\n",
    "df_scaled.index = crypto_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for text features\n",
    "df_variables = pd.get_dummies(crypto_data, columns=['Algorithm', 'ProofType'])\n",
    "df_variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaled = StandardScaler().fit_transform(df_variables)\n",
    "print(scaled[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for text features\n",
    "df_variables = pd.get_dummies(crypto_data, columns=['Algorithm', 'ProofType'])\n",
    "df_variables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect new dataframe with dummy features \n",
    "df_variables.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling \n",
    "* The AWS Sagemaker algorithm for principal component analysis (PCA) was used to reduce the dimensionality of cryptocurrency data. This method decomposes the data matrix into features that are orthogonal with each other. \n",
    "* The resultant orthogonal features are linear combinations of the original feature set. \n",
    "* This method involves taking many features and combining similar or redundant features together to form a new, smaller feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 33\n",
    "\n",
    "pca_SM = PCA(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",  \n",
    "    output_path=\"s3://\" + bucket,\n",
    "    num_components=num_components,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_variables.values.astype(\"float32\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Attributes for Principal Components Analysis (PCA)\n",
    "* Model artifacts are stored in AWS S3 after completing training in previous step. \n",
    "* The model artifact is stored as an ND array. \n",
    "* The model resides in <training_job_name>/output/model.tar.gz file, which is a TAR archive file compressed with GNU zip (gzip) compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = pca_SM.latest_training_job.name\n",
    "model_key = job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).download_file(model_key, \"model.tar.gz\")\n",
    "os.system(\"tar -zxvf model.tar.gz\")\n",
    "\n",
    "# Load ND array via MXNet\n",
    "pca_model_params = mx.ndarray.load(\"model_algo-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the makeup of each PCA component based on the weightings of the original features that are included in the component\n",
    "component_num=3\n",
    "\n",
    "first_comp = v_5[5-component_num]\n",
    "comps = pd.DataFrame(list(zip(first_comp, df_variables.columns)), columns=['weights', 'features'])\n",
    "comps['abs_weights']=comps['weights'].apply(lambda x: np.abs(x))\n",
    "ax=sns.barplot(data=comps.sort_values('abs_weights', ascending=False).head(10), x=\"weights\", y=\"features\", palette=\"Blues_d\")\n",
    "ax.set_title(\"PCA Component Makeup: #\" + str(component_num))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_predictor = pca_SM.deploy(initial_instance_count=1, instance_type=\"ml.c4.xlarge\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_list = [\"principal component 1\", \"principal component 2\", \"principal component 3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass original dataset to model \n",
    "result = pca_predictor.predict(train_data) \n",
    "\n",
    "crypto_data_transformed = pd.DataFrame()\n",
    "\n",
    "for a in result:\n",
    "    b = a.label[\"projection\"].float32_tensor.values\n",
    "    crypto_data_transformed = crypto_data_transformed.append([list(b)])\n",
    "    \n",
    "crypto_data_transformed.index = crypto_data_transformed.index\n",
    "crypto_data_transformed = crypto_data_transformed.iloc[:, 28:]\n",
    "crypto_data_transformed.columns = PCA_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataframe\n",
    "crypto_data_transformed.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Algorithm (Unsupervised Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = crypto_data_transformed.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and define the hyperparameters of model\n",
    "## The KMeans algorithm allows the user to specify how many clusters to identify\n",
    "num_clusters = 3\n",
    "\n",
    "kmeans = KMeans(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=\"s3://\" + bucket + \"/counties/\",\n",
    "    k=num_clusters,\n",
    ")\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The record_set function in the Amazon SageMaker PCA model converts a numpy array into a record set format that is the required format for the input data to be trained.\n",
    "* The use of this data type is one of the reasons that allows training of models within Amazon SageMaker to perform quicker than other implementations such as sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on training data\n",
    "kmeans.fit(kmeans.record_set(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model and pass in the original training set\n",
    "kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type=\"ml.c4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kmeans_predictor.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View breakdown of cluster counts and the distribution of clusters\n",
    "cluster_labels = [r.label[\"closest_cluster\"].float32_tensor.values[0] for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cluster_labels)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster counts\n",
    "ax = plt.subplots(figsize=(6, 3))\n",
    "ax = sns.distplot(cluster_labels, kde=False)\n",
    "title = \"Histogram of Cluster Counts\"\n",
    "ax.set_title(title, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Attributes for KMeans Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = kmeans.latest_training_job.name\n",
    "model_key = job_name + \"/output/model_kmeans.tar.gz\"\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).download_file(model_key, \"model_kmeans.tar.gz\")\n",
    "os.system(\"tar -zxvf model_kmeans.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_model_params = mx.ndarray.load(\"model_algo-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Centroids Locations\n",
    "crypto_data_transformed = pd.DataFrame(Kmeans_model_params[0].asnumpy())\n",
    "crypto_data_transformed.columns = crypto_data_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of cluster centroids\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(cluster_centroids.T, cmap=\"YlGnBu\")\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "ax.set_title(\"Attribute Value by Centroid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_data_transformed[\"labels\"] = list(map(int, cluster_labels))\n",
    "crypto_data_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = crypto_data_transformed[crypto_data_transformed[\"TotalCoinsMined\"] == 1]\n",
    "cluster.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Predictor\n",
    "pca_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Predictor\n",
    "kmeans_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
